[2024-11-23 22:45:27,523] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: colinnyuh (colinnyuh-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/scur2847/.netrc
11/23/2024 22:45:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/23/2024 22:45:38 - INFO - __main__ - Training parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
analysis_dataset=bbh,
analysis_mode=False,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=3,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],
fsdp_config={'fsdp_transformer_layer_cls_to_wrap': ['LlamaDecoderLayer'], 'fsdp_backward_prefetch': 'backward_pre', 'limit_all_gathers': 'true', 'use_orig_params': 'true', 'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/scratch-shared/ir2-less/out/llama2-13b-p0.05-lora-seed3/runs/Nov23_22-45-35_gcn107.local.snellius.surf.nl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=4.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/scratch-shared/ir2-less/out/llama2-13b-p0.05-lora-seed3,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/scratch-shared/ir2-less/out/llama2-13b-p0.05-lora-seed3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=0,
skip_memory_metrics=True,
split_batches=False,
tf32=False,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_dataset_names=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
11/23/2024 22:45:38 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='meta-llama/Llama-2-13b-hf', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None, lora=True, lora_r=128, lora_alpha=512.0, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'])
11/23/2024 22:45:38 - INFO - __main__ - Dataset parameters DataArguments(train_files=['data/train/processed/flan_v2/flan_v2_data.jsonl', 'data/train/processed/cot/cot_data.jsonl', 'data/train/processed/dolly/dolly_data.jsonl', 'data/train/processed/oasst1/oasst1_data.jsonl'], overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=2048, sample_data_seed=42, percentage=0.05)
/home/scur2847/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|tokenization_utils_base.py:2026] 2024-11-23 22:45:40,201 >> loading file tokenizer.model from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2024-11-23 22:45:40,201 >> loading file tokenizer.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2024-11-23 22:45:40,201 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2026] 2024-11-23 22:45:40,201 >> loading file special_tokens_map.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2024-11-23 22:45:40,201 >> loading file tokenizer_config.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/tokenizer_config.json
Using custom data configuration default-0338088e5cfdaa1d
11/23/2024 22:45:40 - INFO - datasets.builder - Using custom data configuration default-0338088e5cfdaa1d
Loading Dataset Infos from /home/scur2847/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
11/23/2024 22:45:40 - INFO - datasets.info - Loading Dataset Infos from /home/scur2847/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
11/23/2024 22:45:40 - INFO - datasets.builder - Generating dataset json (/home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Downloading and preparing dataset json/default to /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...
11/23/2024 22:45:40 - INFO - datasets.builder - Downloading and preparing dataset json/default to /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...
Downloading took 0.0 min
11/23/2024 22:45:40 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
11/23/2024 22:45:40 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Generating train split
11/23/2024 22:45:40 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 59526 examples [00:00, 503141.01 examples/s]Generating train split: 152131 examples [00:00, 728788.89 examples/s]Generating train split: 253732 examples [00:00, 654970.07 examples/s]Generating train split: 270679 examples [00:00, 641762.17 examples/s]
Unable to verify splits sizes.
11/23/2024 22:45:41 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.
11/23/2024 22:45:41 - INFO - datasets.builder - Dataset json downloaded and prepared to /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.
Process #0 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00000_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #0 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00000_of_00010.arrow
Process #1 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00001_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #1 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00001_of_00010.arrow
Process #2 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00002_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #2 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00002_of_00010.arrow
Process #3 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00003_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #3 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00003_of_00010.arrow
Process #4 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00004_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #4 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00004_of_00010.arrow
Process #5 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00005_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #5 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00005_of_00010.arrow
Process #6 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00006_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #6 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00006_of_00010.arrow
Process #7 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00007_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #7 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00007_of_00010.arrow
Process #8 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00008_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #8 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00008_of_00010.arrow
Process #9 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00009_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Process #9 will write at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00009_of_00010.arrow
Spawning 10 processes
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Spawning 10 processes
Tokenizing and reformatting instruction data (num_proc=10):   0%|          | 0/13533 [00:00<?, ? examples/s]Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00000_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00000_of_00010.arrow
Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00001_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00001_of_00010.arrow
Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00002_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00002_of_00010.arrow
Tokenizing and reformatting instruction data (num_proc=10):   0%|          | 21/13533 [00:00<01:54, 117.86 examples/s]Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00003_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00003_of_00010.arrow
Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00004_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00004_of_00010.arrow
Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00005_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00005_of_00010.arrow
Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00006_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00006_of_00010.arrow
Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00007_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00007_of_00010.arrow
Tokenizing and reformatting instruction data (num_proc=10):   2%|‚ñè         | 220/13533 [00:00<00:14, 932.34 examples/s]Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00008_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00008_of_00010.arrow
Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00009_of_00010.arrow
11/23/2024 22:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d675d2ea3405379e_00009_of_00010.arrow
Tokenizing and reformatting instruction data (num_proc=10):   6%|‚ñå         | 765/13533 [00:00<00:04, 2649.50 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  10%|‚ñà         | 1405/13533 [00:00<00:03, 3915.59 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  15%|‚ñà‚ñå        | 2037/13533 [00:00<00:02, 4666.12 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  20%|‚ñà‚ñâ        | 2677/13533 [00:00<00:02, 5192.72 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  24%|‚ñà‚ñà‚ñç       | 3252/13533 [00:00<00:01, 5354.28 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  28%|‚ñà‚ñà‚ñä       | 3836/13533 [00:00<00:01, 5478.00 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  33%|‚ñà‚ñà‚ñà‚ñé      | 4485/13533 [00:00<00:01, 5753.17 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  38%|‚ñà‚ñà‚ñà‚ñä      | 5081/13533 [00:01<00:01, 5636.51 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5679/13533 [00:01<00:01, 5662.69 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6280/13533 [00:01<00:01, 5744.60 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 6907/13533 [00:01<00:01, 5856.53 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 7528/13533 [00:01<00:01, 5919.59 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 8149/13533 [00:01<00:00, 5928.86 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 8780/13533 [00:01<00:00, 5961.51 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9436/13533 [00:01<00:00, 6100.27 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10071/13533 [00:01<00:00, 5269.37 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 10644/13533 [00:02<00:00, 5326.33 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 11254/13533 [00:02<00:00, 5509.31 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 11880/13533 [00:02<00:00, 5701.62 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12507/13533 [00:02<00:00, 5813.23 examples/s]Tokenizing and reformatting instruction data (num_proc=10):  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 13100/13533 [00:02<00:00, 5810.37 examples/s]Tokenizing and reformatting instruction data (num_proc=10): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13533/13533 [00:02<00:00, 4799.09 examples/s]
Concatenating 10 shards
11/23/2024 22:45:44 - INFO - datasets.arrow_dataset - Concatenating 10 shards
[INFO|configuration_utils.py:739] 2024-11-23 22:45:44,575 >> loading configuration file config.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/config.json
[INFO|configuration_utils.py:802] 2024-11-23 22:45:44,578 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-13b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3344] 2024-11-23 22:45:45,135 >> loading weights file model.safetensors from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/model.safetensors.index.json
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:47<01:34, 47.26s/it]Downloading shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [01:34<00:46, 46.99s/it]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:03<00:00, 38.87s/it]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [02:03<00:00, 41.09s/it]
[INFO|configuration_utils.py:826] 2024-11-23 22:47:48,415 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:09,  4.69s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:09<00:04,  4.67s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  3.80s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.03s/it]
[INFO|modeling_utils.py:4185] 2024-11-23 22:48:00,678 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4193] 2024-11-23 22:48:00,678 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-13b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:781] 2024-11-23 22:48:00,914 >> loading configuration file generation_config.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/generation_config.json
[INFO|configuration_utils.py:826] 2024-11-23 22:48:00,914 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|modeling_utils.py:1813] 2024-11-23 22:48:00,920 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
11/23/2024 22:48:05 - INFO - __main__ - Applied LoRA to model.
trainable params: 209,715,200 || all params: 13,225,589,760 || trainable%: 1.5856774919351497
Map:   0%|          | 0/13533 [00:00<?, ? examples/s]/gpfs/home1/scur2847/ir2-less-data/less/train/data_arguments.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  com_len = (torch.tensor(labels) > -1).sum()
Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d15773758665ddf1.arrow
11/23/2024 22:48:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-0338088e5cfdaa1d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d15773758665ddf1.arrow
Map:   7%|‚ñã         | 1000/13533 [00:00<00:01, 7727.35 examples/s]Map:  15%|‚ñà‚ñç        | 2000/13533 [00:00<00:01, 8712.35 examples/s]Map:  22%|‚ñà‚ñà‚ñè       | 3000/13533 [00:00<00:01, 8768.89 examples/s]Map:  30%|‚ñà‚ñà‚ñâ       | 4000/13533 [00:00<00:01, 9117.47 examples/s]Map:  37%|‚ñà‚ñà‚ñà‚ñã      | 5000/13533 [00:00<00:00, 9139.87 examples/s]Map:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 6000/13533 [00:00<00:00, 9070.29 examples/s]Map:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 7000/13533 [00:00<00:00, 9218.92 examples/s]Map:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8000/13533 [00:00<00:00, 9361.20 examples/s]Map:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 9000/13533 [00:00<00:00, 9296.50 examples/s]Map:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10000/13533 [00:01<00:00, 9233.69 examples/s]Map:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 11000/13533 [00:01<00:00, 8724.18 examples/s]Map:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 12000/13533 [00:01<00:00, 9046.72 examples/s]Map:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13000/13533 [00:01<00:00, 8388.51 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13533/13533 [00:01<00:00, 8794.27 examples/s]
[train set] examples: 13533; # avg tokens: 370.9773254394531
[train set] examples: 13533; # avg completion tokens: 105.39820861816406
11/23/2024 22:48:07 - INFO - __main__ - Sample 6311 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  5618,   338, 13258,
          358,  9124,   292, 29973, 10604, 29901,    13, 29966, 29989,   465,
        22137, 29989, 29958,    13,   797, 10147,   358,  9124,   292,   639,
         2408, 29879,   304,  3058, 14188,   310,   263, 18161,  5786,  5001,
          470,   263, 17266,   403,  8542,   393,  5718,   297, 25228,   706,
        29899,  6707, 18161, 22160,   373,  2306,  3131,   310, 15724, 29892,
        17266,   800, 29892,   322,  4095,  1860, 29889, 18375, 17658,  6942,
          411, 17266,   403,  1436,   749, 29892,  1316,   263,  9124,  1795,
         6985,   297, 29263, 18161,  7483,   491,  1090, 16554,   470, 16684,
          408,   278,  3132, 29915, 29879, 10823,   297,   278, 17759,   749,
          310,  2553, 29873,   470,  1592,   537,   409,  2764,  1907, 29889,
          530, 13258,   358,  9124,  1122,   884,  6985, 14582,  9701,   297,
         2778,  5743,   322,  1274,  7680,  2187,   313, 29924, 29987, 29909,
        29897,   322,  3867, 10359,   453,   653,  5786,  1316,   408,  9999,
         3907, 29892,  3534,   292,   310, 25748,   322,  1592,   537,   409,
         2764,  1907, 29892,   383,  2965, 29907,  5786,   313, 20227, 17869,
        23643, 29892, 16256, 15942, 29892,   322,   844,   397,  1907, 29897,
          470,  5925,   313, 25254, 29872,  4599,   293, 29892, 16200,   470,
         1592,   537,  5925,   467,  7849, 13258,   358, 24388,  7344,  6019,
         2545,  3946,   482,   322, 24342, 10643,  5840,  1860,   297,  9589,
          651,   411,  1009, 13258,   358,  5925,  5381,   267, 29889,  1094,
          385, 13661, 29892,   372,   338,  9391,   701,   964,   278,  8313,
          479,  5032,  3522,   313, 21064, 26485,   511, 14253, 28794,   313,
         6563, 29899,  5563,  5381,   267,   511,   322, 25927,  1387,  9999,
          313, 18732,  1891,  5381,   267,   467,    13,    13,  2525,  4561,
        12128, 24388,   322,  3240,   737, 24388, 29892, 13258,   358, 24388,
          437,   451,  2125, 19754,  1169, 29889,     2, 29871,    13]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,   797, 10147,   358,  9124,   292,   639,
         2408, 29879,   304,  3058, 14188,   310,   263, 18161,  5786,  5001,
          470,   263, 17266,   403,  8542,   393,  5718,   297, 25228,   706,
        29899,  6707, 18161, 22160,   373,  2306,  3131,   310, 15724, 29892,
        17266,   800, 29892,   322,  4095,  1860, 29889, 18375, 17658,  6942,
          411, 17266,   403,  1436,   749, 29892,  1316,   263,  9124,  1795,
         6985,   297, 29263, 18161,  7483,   491,  1090, 16554,   470, 16684,
          408,   278,  3132, 29915, 29879, 10823,   297,   278, 17759,   749,
          310,  2553, 29873,   470,  1592,   537,   409,  2764,  1907, 29889,
          530, 13258,   358,  9124,  1122,   884,  6985, 14582,  9701,   297,
         2778,  5743,   322,  1274,  7680,  2187,   313, 29924, 29987, 29909,
        29897,   322,  3867, 10359,   453,   653,  5786,  1316,   408,  9999,
         3907, 29892,  3534,   292,   310, 25748,   322,  1592,   537,   409,
         2764,  1907, 29892,   383,  2965, 29907,  5786,   313, 20227, 17869,
        23643, 29892, 16256, 15942, 29892,   322,   844,   397,  1907, 29897,
          470,  5925,   313, 25254, 29872,  4599,   293, 29892, 16200,   470,
         1592,   537,  5925,   467,  7849, 13258,   358, 24388,  7344,  6019,
         2545,  3946,   482,   322, 24342, 10643,  5840,  1860,   297,  9589,
          651,   411,  1009, 13258,   358,  5925,  5381,   267, 29889,  1094,
          385, 13661, 29892,   372,   338,  9391,   701,   964,   278,  8313,
          479,  5032,  3522,   313, 21064, 26485,   511, 14253, 28794,   313,
         6563, 29899,  5563,  5381,   267,   511,   322, 25927,  1387,  9999,
          313, 18732,  1891,  5381,   267,   467,    13,    13,  2525,  4561,
        12128, 24388,   322,  3240,   737, 24388, 29892, 13258,   358, 24388,
          437,   451,  2125, 19754,  1169, 29889,     2, 29871,    13]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1])}.
11/23/2024 22:48:07 - INFO - __main__ - trainable model_params: 209715200
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 5120)
        (layers): ModuleList(
          (0-39): 40 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=5120, out_features=32001, bias=False)
    )
  )
)
/home/scur2847/.local/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[INFO|trainer.py:568] 2024-11-23 22:48:07,831 >> Using auto half precision backend
start training!!!!
[INFO|trainer.py:1706] 2024-11-23 22:48:12,079 >> ***** Running training *****
[INFO|trainer.py:1707] 2024-11-23 22:48:12,080 >>   Num examples = 13,533
[INFO|trainer.py:1708] 2024-11-23 22:48:12,080 >>   Num Epochs = 4
[INFO|trainer.py:1709] 2024-11-23 22:48:12,080 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1712] 2024-11-23 22:48:12,080 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1713] 2024-11-23 22:48:12,080 >>   Gradient Accumulation steps = 32
[INFO|trainer.py:1714] 2024-11-23 22:48:12,080 >>   Total optimization steps = 1,688
[INFO|trainer.py:1715] 2024-11-23 22:48:12,083 >>   Number of trainable parameters = 209,715,200
[INFO|integration_utils.py:722] 2024-11-23 22:48:12,087 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /gpfs/home1/scur2847/ir2-less-data/wandb/run-20241123_224812-to6lxja9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-snowflake-39
wandb: ‚≠êÔ∏è View project at https://wandb.ai/colinnyuh-university-of-amsterdam/huggingface
wandb: üöÄ View run at https://wandb.ai/colinnyuh-university-of-amsterdam/huggingface/runs/to6lxja9
  0%|          | 0/1688 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-23 22:48:13,129 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 191, in <module>
    main()
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 170, in main
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2744, in training_step
    self.accelerator.backward(loss)
  File "/home/scur2847/.local/lib/python3.11/site-packages/accelerate/accelerator.py", line 2196, in backward
    loss.backward(**kwargs)
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 784, in _post_backward_hook
    _reduce_grad_no_shard(state, handle)
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 940, in _reduce_grad_no_shard
    _cast_grad_to_param_dtype(state, flat_param.grad, flat_param)
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 1041, in _cast_grad_to_param_dtype
    sharded_grad.data = sharded_grad.data.to(dtype=param.dtype)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacty of 93.12 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 91.99 GiB memory in use. Of the allocated memory 90.34 GiB is allocated by PyTorch, and 733.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1;34mwandb[0m: üöÄ View run [33mwoven-snowflake-39[0m at: [34mhttps://wandb.ai/colinnyuh-university-of-amsterdam/huggingface/runs/to6lxja9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241123_224812-to6lxja9/logs[0m
[2024-11-23 22:48:17,834] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2705903) of binary: /sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/bin/python
Traceback (most recent call last):
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
less.train.train FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-23_22:48:17
  host      : gcn107.local.snellius.surf.nl
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2705903)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

JOB STATISTICS
==============
Job ID: 8659252
Cluster: snellius
User/Group: scur2847/scur2847
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 32
CPU Utilized: 00:03:48
CPU Efficiency: 3.73% of 01:41:52 core-walltime
Job Wall-clock time: 00:03:11
Memory Utilized: 4.88 GB
Memory Efficiency: 1.36% of 360.00 GB
